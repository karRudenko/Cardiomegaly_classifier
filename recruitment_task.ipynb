{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50c77e44",
   "metadata": {},
   "source": [
    "# Machine Learning Model Comparison for Cardiomegaly Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe513dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV, RepeatedStratifiedKFold, cross_val_score, StratifiedKFold, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc40e641",
   "metadata": {},
   "source": [
    "##Problem Overview\n",
    "This project focuses on building machine learning models to detect cardiomegaly (enlarged heart) from medical imaging features. Cardiomegaly is a medical condition where the heart is enlarged, which can be identified through various measurements derived from chest imaging.\n",
    "\n",
    "The dataset contains various geometric and morphological features extracted from medical images, and our goal is to classify whether a patient has cardiomegaly based on these features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc39f44a",
   "metadata": {},
   "source": [
    "## Dataset Description\n",
    "\n",
    "The dataset includes the following key features:\n",
    "\n",
    "- **Heart width**: Width measurement of the heart\n",
    "- **Lung width**: Width measurement of the lungs  \n",
    "- **CTR - Cardiothoracic Ratio**: Ratio of heart width to chest width\n",
    "- **Geometric features**: xx, yy, xy coordinates and normalized differences\n",
    "- **Shape descriptors**: Inscribed circle radius, polygon area ratio\n",
    "- **Size measurements**: Heart perimeter, heart area, lung area\n",
    "\n",
    "The target variable \"Cardiomegaly\" indicates whether the patient has an enlarged heart (1) or not (0)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df013ff",
   "metadata": {},
   "source": [
    "## Data Loading and Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd4184c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"src/task_data.csv\")\n",
    "# some data are string type, so we need to make them float type\n",
    "for col in data.columns:\n",
    "    if data[col].dtype == 'object': \n",
    "        data[col] = data[col].str.replace(',', '.').astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d832612",
   "metadata": {},
   "source": [
    "The data is loaded from a CSV file, and any string columns (likely containing numbers with comma as decimal separator) are converted to float format for numerical processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6c07a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[[\n",
    "   'Heart width', 'Lung width', 'CTR - Cardiothoracic Ratio', 'xx', 'yy', 'xy', 'normalized_diff', 'Inscribed circle radius', 'Polygon Area Ratio', 'Heart perimeter', 'Heart area ', 'Lung area'\n",
    "]]                          # features\n",
    "y = data[\"Cardiomegaly\"]    #target column\n",
    "\n",
    "# training 80%, testing 20%\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler();\n",
    "X_scaled_train = scaler.fit_transform(X_train)\n",
    "X_scaled_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67be43a1",
   "metadata": {},
   "source": [
    "We select relevant medical imaging features and split the data into training (80%) and testing (20%) sets, ensuring reproducible results with a fixed random state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41ffb25",
   "metadata": {},
   "source": [
    "## Machine Learning Models\n",
    "\n",
    "Was implemented and compared five different classification algorithms:\n",
    "- K-Nearest Neighbors (KNN)\n",
    "- Decision Tree\n",
    "- Support Vector Machine (SVM)\n",
    "- Logistic Regression\n",
    "- Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c17d7f",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors (KNN)\n",
    "\n",
    "We begin with the K-Nearest Neighbors classifier.\n",
    "To find the optimal hyperparameters, we use GridSearchCV combined with RepeatedStratifiedKFold, ensuring that results are both reliable and statistically stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5336b727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 500 folds for each of 3 candidates, totalling 1500 fits\n",
      "Best accuracy (averaged CV): 0.8406\n"
     ]
    }
   ],
   "source": [
    "# K-Nearest Neighbors classifier\n",
    "param_grid = {\n",
    "    \"model__n_neighbors\": [6, 7, 8],  \n",
    "    \"model__weights\": [\"uniform\"],  \n",
    "    \"model__metric\": [\"manhattan\"], \n",
    "}\n",
    "rskf = RepeatedStratifiedKFold(\n",
    "    n_splits=5,\n",
    "    n_repeats=100,\n",
    "    random_state=None\n",
    ")\n",
    "pipe_knn = Pipeline(steps=[\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"model\", KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=pipe_knn,       \n",
    "    param_grid=param_grid,      \n",
    "    scoring=\"accuracy\",         \n",
    "    cv=rskf,                    \n",
    "    verbose=1,                  \n",
    "    n_jobs=-1                    \n",
    ")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best accuracy (averaged CV): {grid_search.best_score_:.4f}\")\n",
    "the_best = grid_search.best_score_, \"K-Nearest Neighbors\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fb85e0",
   "metadata": {},
   "source": [
    "After tuning, the best average cross-validation accuracy(84%) is printed, and this model becomes our first candidate for comparison. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a4481d",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367bdbb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cross-validation mean score: 0.786\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree\n",
    "clf_tree = DecisionTreeClassifier(\n",
    "    max_depth=7, \n",
    "    criterion=\"log_loss\",\n",
    "    min_samples_split=7,\n",
    "    min_samples_leaf=4,\n",
    "    class_weight=None\n",
    ")\n",
    "\n",
    "clf_tree.fit(X_train, y_train)\n",
    "\n",
    "cv_score = np.round(cross_val_score(clf_tree, X_train, y_train), 2)\n",
    "\n",
    "print(f\"\\nCross-validation mean score: {np.mean(cv_score):.3}\")\n",
    "if(the_best[0] < np.mean(cv_score)):\n",
    "    the_best = np.mean(cv_score), \"Decision Tree\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a50471",
   "metadata": {},
   "source": [
    "## Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "991dbab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cross-validation mean score: 0.866\n"
     ]
    }
   ],
   "source": [
    "# Support Vector Machine (SVM)\n",
    "pipe_svc = Pipeline(steps=[\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"model\", SVC(\n",
    "        kernel=\"rbf\",\n",
    "        C=3,\n",
    "        gamma=\"scale\",\n",
    "        class_weight=None\n",
    "    ))\n",
    "])\n",
    "\n",
    "pipe_svc.fit(X_train, y_train)\n",
    "\n",
    "cv_score = np.round(cross_val_score(pipe_svc, X_train, y_train), 2)\n",
    "print(f\"\\nCross-validation mean score: {np.mean(cv_score):.3}\")\n",
    "if(the_best[0] < np.mean(cv_score)):\n",
    "    the_best = np.mean(cv_score), \"SVM\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fc7109",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "765c82b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cross-validation mean score: 0.760\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "pipe_log = Pipeline(steps=[\n",
    "    (\"scaler\", StandardScaler()),       \n",
    "    (\"model\", LogisticRegression(        \n",
    "        C=1,                            \n",
    "        penalty=\"l1\",                      \n",
    "        solver=\"liblinear\",                \n",
    "        max_iter=1000,                     \n",
    "        class_weight=None                  \n",
    "    ))\n",
    "])\n",
    "\n",
    "pipe_log.fit(X_train, y_train)\n",
    "\n",
    "cv_score = np.round(cross_val_score(pipe_log, X_train, y_train), 2)\n",
    "\n",
    "print(f\"\\nCross-validation mean score: {cv_score.mean():.3f}\")\n",
    "if(the_best[0] < np.mean(cv_score)):\n",
    "    the_best = np.mean(cv_score), \"Logistic Regression\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8b88c8",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53d3eb90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cross-validation mean score: 0.898\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Classifier\n",
    "clf_rf = RandomForestClassifier(\n",
    "    max_depth=6,                  \n",
    "    min_samples_split=6,           \n",
    "    n_estimators=125,              \n",
    "    min_samples_leaf=2,             \n",
    "    max_features='sqrt'            \n",
    ")\n",
    "\n",
    "clf_rf.fit(X_train, y_train)\n",
    "\n",
    "cv_score = np.round(cross_val_score(clf_rf, X_train, y_train), 2)\n",
    "\n",
    "print(f\"\\nCross-validation mean score: {np.mean(cv_score):.3f}\")\n",
    "if(the_best[0] < np.mean(cv_score)):\n",
    "    the_best = np.mean(cv_score), \"Random Forest Classifier\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28d766a",
   "metadata": {},
   "source": [
    "## Models score\n",
    "\n",
    "After evaluating all models using cross-validation, we compare their average accuracies to identify the best-performing method:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f4eb31",
   "metadata": {},
   "source": [
    "| Classifier | Accuracy |\n",
    "|---|---|\n",
    "| KNN | 84.06% |\n",
    "| Decision Tree | 78.6% |\n",
    "| SVM | 86.6% |\n",
    "| Logistic Regression | 76.0% |\n",
    "| Random Forest | 89.8% |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71394dff",
   "metadata": {},
   "source": [
    "The model achieving the highest cross-validation accuracy is stored as the best performing approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "71196501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the highest accuracy: 89.80%, the method: Random Forest Classifier\n"
     ]
    }
   ],
   "source": [
    "print(f\"the highest accuracy: {the_best[0]*100:.2f}%, the method: {the_best[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82c9e6a",
   "metadata": {},
   "source": [
    "## Running the Model on the Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9945a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set:\n",
      "- Accuracy of KNN Classifier model on test dataset: 0.7500\n",
      "- Accuracy of SVC model on test dataset: 0.7500\n",
      "- Accuracy of Logistic Regression model on test dataset: 0.7500\n",
      "- Accuracy of Decision Tree model on test dataset: 0.5000\n",
      "- Accuracy of Random Forest model on test dataset: 0.6250\n"
     ]
    }
   ],
   "source": [
    "# Model on the Test Dataset\n",
    "pipe_knn.fit(X_train, y_train) # haven't done it yet\n",
    "y_pred_knn = pipe_knn.predict(X_test)\n",
    "y_pred_svc = pipe_svc.predict(X_test)\n",
    "y_pred_log = pipe_log.predict(X_test)\n",
    "y_pred_tree = clf_tree.predict(X_test)\n",
    "y_pred_rf = clf_rf.predict(X_test)\n",
    "\n",
    "# Model evaluation: calculate accuracy for each model separately\n",
    "acc_knn = accuracy_score(y_test, y_pred_knn)\n",
    "acc_svc = accuracy_score(y_test, y_pred_svc)\n",
    "acc_log = accuracy_score(y_test, y_pred_log)\n",
    "acc_tree = accuracy_score(y_test, y_pred_tree)\n",
    "acc_rf = accuracy_score(y_test, y_pred_rf)\n",
    "\n",
    "print(f\"Accuracy on test set:\")\n",
    "print(f\"- Accuracy of KNN Classifier model on test dataset: {acc_knn:.4f}\")\n",
    "print(f\"- Accuracy of SVC model on test dataset: {acc_svc:.4f}\")\n",
    "print(f\"- Accuracy of Logistic Regression model on test dataset: {acc_log:.4f}\")\n",
    "print(f\"- Accuracy of Decision Tree model on test dataset: {acc_tree:.4f}\")\n",
    "print(f\"- Accuracy of Random Forest model on test dataset: {acc_rf:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63aec42d",
   "metadata": {},
   "source": [
    "## Final Remarks\n",
    "\n",
    "From these results, it is evident that KNN, SVC, and Logistic Regression achieved the highest and identical accuracy (75%) on the test set, indicating that they generalize similarly well to unseen data.\n",
    "\n",
    "The Decision Tree model performed significantly worse, suggesting potential overfitting or limited depth to capture the underlying patterns. The Random Forest, while generally robust, achieved only moderate accuracy (62.5%), possibly due to suboptimal hyperparameters or insufficient feature diversity.\n",
    "\n",
    "In conclusion, KNN, SVC, and Logistic Regression currently offer the most reliable predictive performance for detecting Cardiomegaly based on the available image-derived measurements."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
